{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a25678c9993c1c2bbf0167f2ff03c982",
     "grade": false,
     "grade_id": "header-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tips\n",
    "- To avoid unpleasant surprises, I suggest you _run all cells in their order of appearance_ (__Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "\n",
    "- If the changes you've made to your solution don't seem to be showing up, try running __Kernel__ $\\rightarrow$ __Restart & Run All__ from the menu.\n",
    "\n",
    "\n",
    "- Before submitting your assignment, make sure everything runs as expected. First, restart the kernel (from the menu, select __Kernel__ $\\rightarrow$ __Restart__) and then **run all cells** (from the menu, select __Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "## Reminder\n",
    "\n",
    "- Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, UA email, and collaborators below:\n",
    "\n",
    "\n",
    "\n",
    "Several of the cells in this notebook are **read only** to ensure instructions aren't unintentionally altered.  \n",
    "\n",
    "If you can't edit the cell, it is probably intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ---
# spaCy NER Tutorial (Beginner-Friendly)
---

# 🧠 1. Introduction to Named Entity Recognition
# Named Entity Recognition (NER) is a process in Natural Language Processing (NLP) where we identify and classify
# named entities in text into categories such as persons, organizations, locations, dates, etc.

# 🧰 2. Install and Import Dependencies
import spacy
from spacy import displacy

# 📥 3. Load a Pre-trained spaCy Model
nlp = spacy.load("en_core_web_sm")

# 📝 4. Input Sample Text
doc = nlp("Apple is looking at buying a U.K. startup for $1 billion. Elon Musk founded SpaceX in 2002.")

# 🔍 5. Named Entities Recognition Results
for ent in doc.ents:
    print(f"{ent.text:30} -> {ent.label_} ({spacy.explain(ent.label_)})")

# 👁️ 6. Visualize the Named Entities
displacy.render(doc, style="ent", jupyter=True)

# 🛠️ 7. Customize NER: Add New Entity Labels
# This part would typically involve training, but here’s how you can add patterns for specific matches
from spacy.pipeline import EntityRuler

ruler = nlp.add_pipe("entity_ruler", before="ner")
ruler.add_patterns([
    {"label": "FOUNDER", "pattern": "Elon Musk"},
    {"label": "COMPANY", "pattern": "SpaceX"},
])

custom_doc = nlp("Elon Musk is the founder of SpaceX and Tesla.")
displacy.render(custom_doc, style="ent", jupyter=True)

# 💡 8. Practical Use Case: Extracting Company Names
example_text = "Google, Microsoft, and OpenAI are leading companies in AI research."
doc = nlp(example_text)

companies = [ent.text for ent in doc.ents if ent.label_ == "ORG"]
print("Extracted Companies:", companies)

# 📘 9. Conclusion
# spaCy makes it easy to get started with NER, and you can extend it with rule-based or statistical models.
# For production-grade customization, consider training spaCy models with your own labeled data.
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0783621da2f047c6360f2ec0d56f121c",
     "grade": false,
     "grade_id": "cell-e35b85c2416e40f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scratchpad\n",
    "\n",
    "You are welcome to create new cells (see the __Cell__ menu) to experiment and debug your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80ac423030cfa372644d7cd456061af",
     "grade": false,
     "grade_id": "cell-955f8133afe96b26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e07f9ac61f4be6a57b6961cde23a6d58",
     "grade": false,
     "grade_id": "cell-a2292c2fbc4cf52e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mini Python tutorial\n",
    "\n",
    "This course uses Python 3.8.\n",
    "\n",
    "Below is a very basic (and incomplete) overview of the Python language... \n",
    "\n",
    "For those completely new to Python, [this section of the official documentation may be useful](https://docs.python.org/3.8/library/stdtypes.html#common-sequence-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfcd9f827d4855d02514b2e54ba32077",
     "grade": false,
     "grade_id": "cell-d6593132353238c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a comment.  \n",
    "# Any line starting with # will be interpreted as a comment\n",
    "\n",
    "# this is a string assigned to a variable\n",
    "greeting = \"hello\"\n",
    "\n",
    "# If enclosed in triple quotes, strings can also be multiline:\n",
    "\n",
    "\"\"\"\n",
    "I'm a multiline\n",
    "string.\n",
    "\"\"\"\n",
    "\n",
    "# let's use a for loop to print it letter by letter\n",
    "for letter in greeting:\n",
    "    print(letter)\n",
    "    \n",
    "# Did you notice the indentation there?  Whitespace matters in Python!\n",
    "\n",
    "# here's a list of integers\n",
    "\n",
    "numbers = [1, 2, 3, 4]\n",
    "\n",
    "# let's add one to each number using a list comprehension\n",
    "# and assign the result to a variable called res\n",
    "# list comprehensions are used widely in Python (they're very Pythonic!)\n",
    "\n",
    "res = [num + 1 for num in numbers]\n",
    "\n",
    "# let's confirm that it worked\n",
    "print(res)\n",
    "\n",
    "# now let's try spicing things up using a conditional to filter out all values greater than or equal to 3...\n",
    "print([num for num in res if not num >= 3])\n",
    "\n",
    "# Python 3.7 introduced \"f-strings\" as a convenient way of formatting strings using templates\n",
    "# For example ...\n",
    "name = \"Josuke\"\n",
    "\n",
    "print(f\"{greeting}, {name}!\")\n",
    "\n",
    "# f-strings are f-ing convenient!\n",
    "\n",
    "\n",
    "# let's look at defining functions in Python..\n",
    "\n",
    "def greet(name):\n",
    "    print(f\"Howdy, {name}!\")\n",
    "\n",
    "# here's how we call it...\n",
    "\n",
    "greet(\"partner\")\n",
    "\n",
    "# let's add a description of the function...\n",
    "\n",
    "def greet(name):\n",
    "    \"\"\"\n",
    "    Prints a greeting given some name.\n",
    "    \n",
    "    :param name: the name to be addressed in the greeting\n",
    "    :type name: str\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Howdy, {name}!\")\n",
    "    \n",
    "# I encourage you to use docstrings!\n",
    "\n",
    "# Python introduced support for optional type hints in v3.5.\n",
    "# You can read more aobut this feature here: https://docs.python.org/3.8/library/typing.html\n",
    "# let's give it a try...\n",
    "def add_six(num: int) -> int:\n",
    "    return num + 6\n",
    "\n",
    "# this should print 13\n",
    "print(add_six(7))\n",
    "\n",
    "# Python also has \"anonymous functions\" (also known as \"lambda\" functions)\n",
    "# take a look at the following code:\n",
    "\n",
    "greet_alt = lambda name: print(f\"Hi, {name}!\")\n",
    "\n",
    "greet_alt(\"Fred\")\n",
    "\n",
    "# lambda functions are often passed to other functions\n",
    "# For example, they can be used to specify how a sequence should be sorted\n",
    "# let's sort a list of pairs by their second element\n",
    "pairs = [(\"bounce\", 32), (\"bighorn\", 12), (\"radical\", 4), (\"analysis\", 7)]\n",
    "# -1 is last thing in some sequence, -2 is the second to last thing in some seq, etc.\n",
    "print(sorted(pairs, key=lambda pair: pair[-1]))\n",
    "\n",
    "# we can sort it by the first element instead\n",
    "# NOTE: python indexing is zero-based\n",
    "print(sorted(pairs, key=lambda pair: pair[0]))\n",
    "\n",
    "# You can learn more about other core data types and their methods here: \n",
    "# https://docs.python.org/3.8/library/stdtypes.html\n",
    "\n",
    "# Because of its extensive standard library, Python is often described as coming with \"batteries included\".  \n",
    "# Take a look at these \"batteries\": https://docs.python.org/3.8/library/\n",
    "\n",
    "# You now know enough to complete this homework assignment (or at least where to look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ea695a2cdac2694e9b7046c1e4a740d",
     "grade": false,
     "grade_id": "test-imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import defaultdict, deque # Think about why these might be helpful\n",
    "from enum        import Enum\n",
    "from typing      import Callable, Iterator, Sequence, Optional, Text, Union\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing      import LabelEncoder\n",
    "from sklearn.linear_model       import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using print, consider using logging.warn, logging.info, logging,debug, logging.error, etc.\n",
    "# setting the logging level here establishes what statements should be silenced/included\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4aff39917eaac1542fada13420103f38",
     "grade": false,
     "grade_id": "cell-130aa7956774e749",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This has been provided for you.  You don't need to  alter it.\n",
    "\n",
    "class ArcStandardAction(Enum):\n",
    "    \"\"\"\n",
    "    An action in an \"arc standard\" transition-based parser.\n",
    "    \"\"\"\n",
    "    SHIFT     = 1\n",
    "    LEFT_ARC  = 2\n",
    "    RIGHT_ARC = 3\n",
    "    \n",
    "    @staticmethod\n",
    "    def get(action_name) -> Optional[Enum]:\n",
    "        return ArcStandardAction._member_map_.get(action_name, None)\n",
    "        \n",
    "\n",
    "# type alias\n",
    "Action = ArcStandardAction\n",
    "\n",
    "# usage:\n",
    "logging.debug(Action(3))\n",
    "logging.debug(Action.get(\"SHIFT\"))\n",
    "logging.debug(Action.get(\"XYLOPHONE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3044fd7b6dff28f378a471d9fc11dd62",
     "grade": false,
     "grade_id": "md-parser",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Implement a transition-based dependency parser \n",
    "\n",
    "Using the arc standard transition system and the provided code skeleton, implement a transition-based dependency parser.\n",
    "\n",
    "## `read_conllu`\n",
    "\n",
    "- A function that reads in .conllu data and creates a sequence of `Dep` instances based on that data.\n",
    "\n",
    "*HINT: Can you make use of any code from previous assignments?*\n",
    "\n",
    "## `parse`\n",
    "\n",
    "\n",
    "## `Oracle`\n",
    "\n",
    "## `Classifier`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05425df28257aea885d5e864e4613523",
     "grade": false,
     "grade_id": "code-parser",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@dataclass()\n",
    "class Dep:\n",
    "    \"\"\"A word in a dependency tree.\n",
    "\n",
    "    The fields are defined by https://universaldependencies.org/format.html.\n",
    "    \"\"\"\n",
    "    id: Text\n",
    "    form: Union[Text, None]\n",
    "    lemma: Union[Text, None]\n",
    "    upos: Text\n",
    "    xpos: Union[Text, None]\n",
    "    feats: Sequence[Text]\n",
    "    head: Union[Text, None]\n",
    "    deprel: Union[Text, None]\n",
    "    deps: Sequence[Text]\n",
    "    misc: Union[Text, None]\n",
    "\n",
    "\n",
    "def read_conllu(path: Text) -> Iterator[Sequence[Dep]]:\n",
    "    \"\"\"\n",
    "    Reads a CoNLL-U format file into sequences of Dep objects.\n",
    "\n",
    "    The CoNLL-U format is described in detail here:\n",
    "    https://universaldependencies.org/format.html\n",
    "    A few key highlights:\n",
    "    * Word lines contain 10 fields separated by tab characters.\n",
    "    * Blank lines mark sentence boundaries.\n",
    "    * Comment lines start with hash (#).\n",
    "\n",
    "    Each word line will be converted into a Dep object, and the words in a\n",
    "    sentence will be collected into a sequence (e.g., list).\n",
    "\n",
    "    :return: An iterator over sentences, where each sentence is a sequence of\n",
    "    words, and each word is represented by a Dep object.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def parse(\n",
    "    deps: Sequence[Dep],\n",
    "    get_action: Callable[[Sequence[Dep], Sequence[Dep]], Action]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Parse the sentence based on \"arc standard\" transitions.\n",
    "\n",
    "    Following the \"arc standard\" approach to transition-based parsing, this\n",
    "    method creates a stack and a queue, where the input Deps start out on the\n",
    "    queue, are moved to the stack by SHIFT actions, and are combined in\n",
    "    head-dependent relations by LEFT_ARC and RIGHT_ARC actions.\n",
    "\n",
    "    This method does not determine which actions to take; those are provided by\n",
    "    the `get_action` argument to the method. That method will be called whenever\n",
    "    the parser needs a new action, and then the parser will perform whatever\n",
    "    action is returned. If `get_action` returns an invalid action (e.g., a\n",
    "    SHIFT when the queue is empty), an arbitrary valid action will be taken\n",
    "    instead.\n",
    "\n",
    "    This method does not return anything; it modifies the `.head` field of the\n",
    "    Dep objects that were passed as input. Each Dep object's `.head` field is\n",
    "    assigned the value of its head's `.id` field, or \"0\" if the Dep object is\n",
    "    the root.\n",
    "\n",
    "    :param deps: The sentence, a sequence of Dep objects, each representing one\n",
    "    of the words in the sentence.\n",
    "    :param get_action: a function or other callable that takes the parser's\n",
    "    current stack and queue as input, and returns an \"arc standard\" action.\n",
    "    \n",
    "    :return: Nothing; the `.head` fields of the input Dep objects are modified.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "class Oracle:\n",
    "    def __init__(self, deps: Sequence[Dep]):\n",
    "        \"\"\"Initializes an Oracle to be used for the given sentence.\n",
    "\n",
    "        Minimally, it initializes a member variable `actions`, a list that\n",
    "        will be updated every time `__call__` is called and a new action is\n",
    "        generated.\n",
    "\n",
    "        Note: a new Oracle object should be created for each sentence; an\n",
    "        Oracle object should not be re-used for multiple sentences.\n",
    "\n",
    "        :param deps: The sentence, a sequence of Dep objects, each representing\n",
    "        one of the words in the sentence.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, stack: Sequence[Dep], queue: Sequence[Dep]) -> Action:\n",
    "        \"\"\"\n",
    "        Returns the Oracle action for the given \"arc standard\" parser state.\n",
    "\n",
    "        The oracle for an \"arc standard\" transition-based parser inspects the\n",
    "        parser state and the reference parse (represented by the `.head` fields\n",
    "        of the Dep objects) and:\n",
    "        * Chooses LEFT_ARC if it produces a correct head-dependent relation\n",
    "          given the reference parse and the current configuration.\n",
    "        * Otherwise, chooses RIGHT_ARC if it produces a correct head-dependent\n",
    "          relation given the reference parse and all of the dependents of the\n",
    "          word at the top of the stack have already been assigned.\n",
    "        * Otherwise, chooses SHIFT.\n",
    "\n",
    "        The chosen action should be both:\n",
    "        * Added to the `actions` member variable\n",
    "        * Returned as the result of this method\n",
    "\n",
    "        Note: this method should only be called on parser state based on the Dep\n",
    "        objects that were passed to __init__; it should not be used for any\n",
    "        other Dep objects.\n",
    "\n",
    "        :param stack: The stack of the \"arc standard\" transition-based parser.\n",
    "        :param queue: The queue of the \"arc standard\" transition-based parser.\n",
    "        \n",
    "        :return: The action that should be taken given the reference parse\n",
    "        (the `.head` fields of the Dep objects).\n",
    "        \"\"\"\n",
    "        # our default action\n",
    "        action = Action.SHIFT\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, parses: Iterator[Sequence[Dep]]):\n",
    "        \"\"\"\n",
    "        Trains a classifier on the given parses.\n",
    "\n",
    "        There are no restrictions on what kind of classifier may be trained,\n",
    "        but a typical approach would be to\n",
    "        1. Define features based on the stack and queue of an \"arc standard\"\n",
    "           transition-based parser (e.g., part-of-speech tags of the top words\n",
    "           in the stack and queue).\n",
    "        2. Apply `Oracle` and `parse` to each parse in the input to generate\n",
    "           training examples of parser states and oracle actions. It may be\n",
    "           helpful to modify `Oracle` to call the feature extraction function\n",
    "           defined in 1, and store the features alongside the actions list that\n",
    "           `Oracle` is already creating.\n",
    "        3. Train a machine learning model (e.g., logistic regression) on the\n",
    "           resulting features and labels (actions).\n",
    "\n",
    "        :param parses: An iterator over sentences, where each sentence is a\n",
    "        sequence of words, and each word is represented by a Dep object.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def _features(stack: Sequence[Dep], queue: Sequence[Dep]):\n",
    "        \"\"\"\n",
    "        Given a stack and queue, Extracts a dictionary of features relevant to a parsing action.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __call__(self, stack: Sequence[Dep], queue: Sequence[Dep]) -> Action:\n",
    "        \"\"\"\n",
    "        Predicts an action for the given \"arc standard\" parser state.\n",
    "\n",
    "        There are no restrictions on how this prediction may be made, but a\n",
    "        typical approach would be to convert the parser state into features,\n",
    "        and then use the machine learning model (trained in `__init__`) to make\n",
    "        the prediction.\n",
    "\n",
    "        :param stack: The stack of the \"arc standard\" transition-based parser.\n",
    "        :param queue: The queue of the \"arc standard\" transition-based parser.\n",
    "        \n",
    "        :return: The action that should be taken.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf7a85efea98f4ca52b16749d0ae3e5c",
     "grade": false,
     "grade_id": "md-test-utils",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test utilities\n",
    "\n",
    "The classses and functions in the cell below are used for running the tests.  You should not alter these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b70b2b3b4c7fbcb0a91dc5c4c1c01a87",
     "grade": false,
     "grade_id": "test-utils",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class IterActions:\n",
    "    \"\"\"\n",
    "    A class for feeding a list of actions, one at a time, to a parser\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: Sequence[Action]):\n",
    "        self.actions_iter = iter(actions)\n",
    "\n",
    "    def __call__(self, stack: Sequence[Dep], queue: Sequence[Dep]):\n",
    "        return next(self.actions_iter)\n",
    "\n",
    "\n",
    "def clear_heads(deps: Sequence[Dep]):\n",
    "    \"\"\"\n",
    "    Removes all head information (.head, .deprel, .deps) from the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    heads = []\n",
    "    for dep in deps:\n",
    "        heads.append(dep.head)\n",
    "        dep.head = None\n",
    "        dep.deprel = None\n",
    "        dep.deps = None\n",
    "    return heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9e6508b6d25353ede50ef1de1b2bcb3",
     "grade": false,
     "grade_id": "md-test-load-conllu",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test loading of CoNLLU data (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89cba8c77a570b1837d3e3bab44c0974",
     "grade": true,
     "grade_id": "test-read-conllu",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_read_conllu():\n",
    "    # read the entire training data and count the words and sentences read\n",
    "    n_sentences = 0\n",
    "    n_deps = 0\n",
    "    for parse in read_conllu(\"data/UD_English-EWT/en_ewt-ud-train.conllu\"):\n",
    "        n_sentences += 1\n",
    "        n_deps += len(parse)\n",
    "\n",
    "        # make sure each sentence is a list of Dep objects\n",
    "        assert all(isinstance(dep, Dep) for dep in parse)\n",
    "\n",
    "        # make sure there is exactly one root node\n",
    "        assert len([dep for dep in parse if dep.deprel == \"root\"]) == 1\n",
    "\n",
    "    # make sure all sentences and words were read\n",
    "    assert n_sentences == 12543\n",
    "    assert n_deps == 204607\n",
    "\n",
    "    # now do a deeper inspection of a single sentence from the training data\n",
    "\n",
    "    # 1\tOver\tover\tADV\tRB\t_\t2\tadvmod\t2:advmod\t_\n",
    "    # 2\t300\t300\tNUM\tCD\tNumType=Card\t3\tnummod\t3:nummod\t_\n",
    "    # 3\tIraqis\tIraqis\tPROPN\tNNPS\tNumber=Plur\t5\tnsubj:pass\t5:nsubj:pass|6:nsubj:xsubj|8:nsubj:pass\t_\n",
    "    # 4\tare\tbe\tAUX\tVBP\tMood=Ind|Tense=Pres|VerbForm=Fin\t5\taux:pass\t5:aux:pass\t_\n",
    "    # 5\treported\treport\tVERB\tVBN\tTense=Past|VerbForm=Part|Voice=Pass\t0\troot\t0:root\t_\n",
    "    # 6\tdead\tdead\tADJ\tJJ\tDegree=Pos\t5\txcomp\t5:xcomp\t_\n",
    "    # 7\tand\tand\tCCONJ\tCC\t_\t8\tcc\t8:cc|8.1:cc\t_\n",
    "    # 8\t500\t500\tNUM\tCD\tNumType=Card\t5\tconj\t5:conj:and|8.1:nsubj:pass|9:nsubj:xsubj\t_\n",
    "    # 8.1\treported\treport\tVERB\tVBN\tTense=Past|VerbForm=Part|Voice=Pass\t_\t_\t5:conj:and\tCopyOf=5\n",
    "    # 9\twounded\twounded\tADJ\tJJ\tDegree=Pos\t8\torphan\t8.1:xcomp\t_\n",
    "    # 10\tin\tin\tADP\tIN\t_\t11\tcase\t11:case\t_\n",
    "    # 11\tFallujah\tFallujah\tPROPN\tNNP\tNumber=Sing\t5\tobl\t5:obl:in\t_\n",
    "    # 12\talone\talone\tADV\tRB\t_\t11\tadvmod\t11:advmod\tSpaceAfter=No\n",
    "    # 13\t.\t.\tPUNCT\t.\t_\t5\tpunct\t5:punct\t_\n",
    "    parses = read_conllu(\"data/UD_English-EWT/en_ewt-ud-train.conllu\")\n",
    "    [parse] = itertools.islice(parses, 61, 62)\n",
    "    assert parse[0] == Dep(\n",
    "        \"1\", \"Over\", \"over\", \"ADV\", \"RB\", [], \"2\", \"advmod\", [\"2:advmod\"], None)\n",
    "    assert parse[2] == Dep(\n",
    "        \"3\", \"Iraqis\", \"Iraqis\", \"PROPN\", \"NNPS\", [\"Number=Plur\"], \"5\",\n",
    "        \"nsubj:pass\", [\"5:nsubj:pass\", \"6:nsubj:xsubj\", \"8:nsubj:pass\"], None)\n",
    "    assert parse[3] == Dep(\n",
    "        \"4\", \"are\", \"be\", \"AUX\", \"VBP\",\n",
    "        [\"Mood=Ind\", \"Tense=Pres\", \"VerbForm=Fin\"],\n",
    "        \"5\", \"aux:pass\", [\"5:aux:pass\"], None)\n",
    "    assert parse[4] == Dep(\n",
    "        \"5\", \"reported\", \"report\", \"VERB\", \"VBN\",\n",
    "        [\"Tense=Past\", \"VerbForm=Part\", \"Voice=Pass\"],\n",
    "        \"0\", \"root\", [\"0:root\"], None)\n",
    "    assert parse[8] == Dep(\n",
    "        \"8.1\", \"reported\", \"report\", \"VERB\", \"VBN\",\n",
    "        [\"Tense=Past\", \"VerbForm=Part\", \"Voice=Pass\"],\n",
    "        None, None,\t[\"5:conj:and\"], \"CopyOf=5\")\n",
    "\n",
    "test_read_conllu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68dcf1591a351c98286a028cd408a09f",
     "grade": true,
     "grade_id": "test-parse",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_parse():\n",
    "    # consider a specific sentence from the training data\n",
    "\n",
    "    # # sent_id = weblog-blogspot.com_alaindewitt_20040929103700_ENG_20040929_103700-0026\n",
    "    # # text = The future president joined the Guard in May 1968.\n",
    "    # 1\tThe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t3:det\t_\n",
    "    # 2\tfuture\tfuture\tADJ\tJJ\tDegree=Pos\t3\tamod\t3:amod\t_\n",
    "    # 3\tpresident\tpresident\tNOUN\tNN\tNumber=Sing\t4\tnsubj\t4:nsubj\t_\n",
    "    # 4\tjoined\tjoin\tVERB\tVBD\tMood=Ind|Tense=Past|VerbForm=Fin\t0\troot\t0:root\t_\n",
    "    # 5\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t6\tdet\t6:det\t_\n",
    "    # 6\tGuard\tGuard\tPROPN\tNNP\tNumber=Sing\t4\tobj\t4:obj\t_\n",
    "    # 7\tin\tin\tADP\tIN\t_\t8\tcase\t8:case\t_\n",
    "    # 8\tMay\tMay\tPROPN\tNNP\tNumber=Sing\t4\tobl\t4:obl:in\t_\n",
    "    # 9\t1968\t1968\tNUM\tCD\tNumType=Card\t8\tnummod\t8:nummod\tSpaceAfter=No\n",
    "    # 10\t.\t.\tPUNCT\t.\t_\t4\tpunct\t4:punct\t_\n",
    "    parses = read_conllu(\"data/UD_English-EWT/en_ewt-ud-train.conllu\")\n",
    "    [deps] = itertools.islice(parses, 352, 353)\n",
    "\n",
    "    # clear out all the head information\n",
    "    orig_heads = clear_heads(deps)\n",
    "\n",
    "    # run the parser with the oracle list of actions\n",
    "    parse(deps, IterActions([\n",
    "        Action.SHIFT,\n",
    "        Action.SHIFT,\n",
    "        Action.SHIFT,\n",
    "        Action.LEFT_ARC,\n",
    "        Action.LEFT_ARC,\n",
    "        Action.SHIFT,\n",
    "        Action.LEFT_ARC,\n",
    "        Action.SHIFT,\n",
    "        Action.SHIFT,\n",
    "        Action.LEFT_ARC,\n",
    "        Action.RIGHT_ARC,\n",
    "        Action.SHIFT,\n",
    "        Action.SHIFT,\n",
    "        Action.LEFT_ARC,\n",
    "        Action.SHIFT,\n",
    "        Action.RIGHT_ARC,\n",
    "        Action.RIGHT_ARC,\n",
    "        Action.SHIFT,\n",
    "        Action.RIGHT_ARC,\n",
    "    ]))\n",
    "\n",
    "    # make sure that the original heads have been restored by the parser\n",
    "    assert [dep.head for dep in deps] == orig_heads\n",
    "test_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a8a88e8f282efe24a48c1eda4c2c97e",
     "grade": false,
     "grade_id": "md-test-oracle",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test oracle (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c9223e5abf81db0fe899456e40d9032",
     "grade": true,
     "grade_id": "test-oracle",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_oracle():\n",
    "    # consider a specific sentence from the training data\n",
    "\n",
    "    # # sent_id = answers-20111108085734AATXy0E_ans-0004\n",
    "    # # text = Plaster of Paris does two things\n",
    "    # 1\tPlaster\tplaster\tNOUN\tNN\tNumber=Sing\t4\tnsubj\t4:nsubj\t_\n",
    "    # 2\tof\tof\tADP\tIN\t_\t3\tcase\t3:case\t_\n",
    "    # 3\tParis\tParis\tPROPN\tNNP\tNumber=Sing\t1\tnmod\t1:nmod:of\t_\n",
    "    # 4\tdoes\tdo\tVERB\tVBZ\tMood=## Test accuracy (3 pts)Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t0:root\t_\n",
    "    # 5\ttwo\ttwo\tNUM\tCD\tNumType=Card\t6\tnummod\t6:nummod\t_\n",
    "    # 6\tthings\tthing\tNOUN\tNNS\tNumber=Plur\t4\tobj\t4:obj\t_\n",
    "    parses = read_conllu(\"data/UD_English-EWT/en_ewt-ud-train.conllu\")\n",
    "    [deps] = itertools.islice(parses, 7475, 7476)\n",
    "\n",
    "    # create an oracle for the sentence and try a few actions\n",
    "    oracle = Oracle(deps)\n",
    "    # shift on an empty stack\n",
    "    assert oracle([], deps) == Action.SHIFT\n",
    "    # shift on a stack with only one entry\n",
    "    assert oracle(deps[:1], deps[1:]) == Action.SHIFT\n",
    "    # shift because \"Plaster\" and \"of\" are not in a head-dependent relation\n",
    "    assert oracle(deps[:2], deps[2:]) == Action.SHIFT\n",
    "    # left-arc because \"Paris\" is the head of \"of\"\n",
    "    assert oracle(deps[:3], deps[3:]) == Action.LEFT_ARC\n",
    "    # right-arc because \"Plaster\" is the head of \"Paris\"\n",
    "    assert oracle(deps[:1] + deps[2:3], deps[3:]) == Action.RIGHT_ARC\n",
    "\n",
    "    # create a new oracle for the same sentence and extract all the actions\n",
    "    oracle = Oracle(deps)\n",
    "    parse(deps, oracle)\n",
    "    assert oracle.actions == [\n",
    "        Action.SHIFT,\n",
    "        Action.SHIFT,\n",
    "        Action.SHIFT,\n",
    "        Action.LEFT_ARC,\n",
    "        Action.RIGHT_ARC,\n",
    "        Action.SHIFT,\n",
    "        Action.LEFT_ARC,\n",
    "        Action.SHIFT,\n",
    "        Action.SHIFT,\n",
    "        Action.LEFT_ARC,\n",
    "        Action.RIGHT_ARC,\n",
    "    ]\n",
    "test_oracle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e54fdaefee7eba5aab7d72750778de69",
     "grade": true,
     "grade_id": "test-oracle-round-trip",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_oracle_round_trip():\n",
    "    # take the first 50 parses from the training data\n",
    "    parses = read_conllu(\"data/UD_English-EWT/en_ewt-ud-train.conllu\")\n",
    "    for i, deps in enumerate(itertools.islice(parses, 50)):\n",
    "\n",
    "        # skip the non-projective parses\n",
    "        if i in {4, 21, 25, 31}:\n",
    "            continue\n",
    "\n",
    "        # collect the head for each word\n",
    "        orig_heads = [dep.head for dep in deps]\n",
    "\n",
    "        # run the oracle to determine the sequence of actions\n",
    "        oracle = Oracle(deps)\n",
    "        parse(deps, oracle)\n",
    "\n",
    "        # clear out all the head information\n",
    "        clear_heads(deps)\n",
    "\n",
    "        # feed the oracle-identified actions in, one at a time\n",
    "        parse(deps, IterActions(oracle.actions))\n",
    "\n",
    "        # make sure that the original heads have been restored by the parser\n",
    "        assert [dep.head for dep in deps] == orig_heads\n",
    "test_oracle_round_trip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f369647ace177d8fcc821f05fd2bfc7",
     "grade": false,
     "grade_id": "md-test-accuracy-base",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test accuracy (3 pts)\n",
    "\n",
    "**WARNING**: this may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12471f66e24dcea8c76b4562274832be",
     "grade": true,
     "grade_id": "test-model-accuracy-base",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def full_model_accuracy():\n",
    "    # train a classifier on the entire training data\n",
    "    train_parses = read_conllu(\"data/UD_English-EWT/en_ewt-ud-train.conllu\")\n",
    "    classifier = Classifier(train_parses)\n",
    "\n",
    "    # test the classifier on the development set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for deps in read_conllu(\"data/UD_English-EWT/en_ewt-ud-dev.conllu\"):\n",
    "        total += len(deps)\n",
    "\n",
    "        # clear out all the head information\n",
    "        orig_heads = clear_heads(deps)\n",
    "\n",
    "        # parse using the classifier to predict actions\n",
    "        parse(deps, classifier)\n",
    "\n",
    "        # count how many of the heads have been correctly restored\n",
    "        for dep, orig_head in zip(deps, orig_heads):\n",
    "            if dep.head == orig_head:\n",
    "                correct += 1\n",
    "\n",
    "    # return the accuracy\n",
    "    return correct / total\n",
    "\n",
    "accuracy   = full_model_accuracy()\n",
    "pretty_acc = accuracy * 100\n",
    "logging.info(f\"{pretty_acc:.2f}% accuracy on EWT development data\")\n",
    "assert accuracy >= 0.625"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
